{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\my_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --------------Load the original dataset while keeping the train-test split--------------------------------------\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "\n",
    "original_dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"originalusername/originaldatasetname\", split=\"train\"),\n",
    "    \"test\": load_dataset(\"originalusername/originaldatasetname\", split=\"test\")\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Push the dataset to your Hugging Face repository-------------------------\n",
    "hf_username = \"yourusername\"  # Change if needed\n",
    "new_dataset_name = \"datasetname\"  # Change if needed\n",
    "dataset_repo = f\"{hf_username}/{new_dataset_name}\"\n",
    "\n",
    "# Create the dataset repo in your account\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=dataset_repo, repo_type=\"dataset\")\n",
    "\n",
    "# Push dataset to your Hugging Face repository\n",
    "original_dataset.push_to_hub(dataset_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting data/train-00040-of-00061.parquet ...\n",
      "Deleted data/train-00040-of-00061.parquet successfully!\n",
      "Deleting data/train-00041-of-00061.parquet ...\n",
      "Deleted data/train-00041-of-00061.parquet successfully!\n",
      "Deleting data/train-00042-of-00061.parquet ...\n",
      "Deleted data/train-00042-of-00061.parquet successfully!\n",
      "Deleting data/train-00043-of-00061.parquet ...\n",
      "Deleted data/train-00043-of-00061.parquet successfully!\n",
      "Deleting data/train-00044-of-00061.parquet ...\n",
      "Deleted data/train-00044-of-00061.parquet successfully!\n",
      "Deleting data/train-00045-of-00061.parquet ...\n",
      "Deleted data/train-00045-of-00061.parquet successfully!\n",
      "Deleting data/train-00046-of-00061.parquet ...\n",
      "Deleted data/train-00046-of-00061.parquet successfully!\n",
      "Deleting data/train-00047-of-00061.parquet ...\n",
      "Deleted data/train-00047-of-00061.parquet successfully!\n",
      "Deleting data/train-00048-of-00061.parquet ...\n",
      "Deleted data/train-00048-of-00061.parquet successfully!\n",
      "Deleting data/train-00049-of-00061.parquet ...\n",
      "Deleted data/train-00049-of-00061.parquet successfully!\n",
      "Deleting data/train-00050-of-00061.parquet ...\n",
      "Deleted data/train-00050-of-00061.parquet successfully!\n",
      "Deleting data/train-00051-of-00061.parquet ...\n",
      "Deleted data/train-00051-of-00061.parquet successfully!\n",
      "Deleting data/train-00052-of-00061.parquet ...\n",
      "Deleted data/train-00052-of-00061.parquet successfully!\n",
      "Deleting data/train-00053-of-00061.parquet ...\n",
      "Deleted data/train-00053-of-00061.parquet successfully!\n",
      "Deleting data/train-00054-of-00061.parquet ...\n",
      "Deleted data/train-00054-of-00061.parquet successfully!\n",
      "Deleting data/train-00055-of-00061.parquet ...\n",
      "Deleted data/train-00055-of-00061.parquet successfully!\n",
      "Deleting data/train-00056-of-00061.parquet ...\n",
      "Deleted data/train-00056-of-00061.parquet successfully!\n",
      "Deleting data/train-00057-of-00061.parquet ...\n",
      "Deleted data/train-00057-of-00061.parquet successfully!\n",
      "Deleting data/train-00058-of-00061.parquet ...\n",
      "Deleted data/train-00058-of-00061.parquet successfully!\n",
      "Deleting data/train-00059-of-00061.parquet ...\n",
      "Deleted data/train-00059-of-00061.parquet successfully!\n",
      "Deleting data/train-00060-of-00061.parquet ...\n",
      "Deleted data/train-00060-of-00061.parquet successfully!\n",
      "All specified files have been processed.\n"
     ]
    }
   ],
   "source": [
    "#----------------------Delete specific training files from your Hugging Face repository to reduce size-------------------------\n",
    "from huggingface_hub import delete_file\n",
    "import time\n",
    "\n",
    "# Define repo ID and list of files to delete\n",
    "repo_id = \"yourusername/datasetname\"  # Your dataset repository\n",
    "file_paths = [f\"data/train-{i:05d}-of-00061.parquet\" for i in range(40, 61)]\n",
    "\n",
    "# Hugging Face token (Replace with your actual token)\n",
    "hf_token = \"<token_id>\"  # Get this from https://huggingface.co/settings/tokens\n",
    "\n",
    "# Loop through files and delete each one\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        print(f\"Deleting {file_path} ...\")\n",
    "        delete_file(\n",
    "            path_in_repo=file_path,\n",
    "            repo_id=repo_id,\n",
    "            repo_type=\"dataset\",  # Ensure we specify it as a dataset repository\n",
    "            token=hf_token\n",
    "        )\n",
    "        time.sleep(1)  # Delay to avoid rate limiting\n",
    "        print(f\"Deleted {file_path} successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "print(\"All specified files have been processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Filtering long sentences in your dataset-------------------------\n",
    "from datasets import DatasetDict, load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Load dataset\n",
    "bangla_dataset = DatasetDict({\n",
    "    \"train\": load_dataset(\"yourusername/datasetname\", split=\"train\"),\n",
    "    \"test\": load_dataset(\"yourusername/datasetname\", split=\"test\")\n",
    "})\n",
    "\n",
    "# Load Whisper processor (includes tokenizer)\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "tokenizer = processor.tokenizer  # Extract tokenizer\n",
    "\n",
    "# Function to filter sentences with token length â‰¤ 448\n",
    "def filter_long_sentences(example):\n",
    "    tokenized = tokenizer(example[\"sentence\"], truncation=False)  # Check real length\n",
    "    return len(tokenized[\"input_ids\"]) <= 448\n",
    "\n",
    "# Apply filtering\n",
    "filtered_test_dataset = bangla_dataset[\"test\"].filter(filter_long_sentences)\n",
    "\n",
    "# Display the new dataset structure\n",
    "print(filtered_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------Save the filtered dataset to disk---------------------------------\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "import os\n",
    "\n",
    "# Save dataset in multiple Parquet files\n",
    "filtered_test_dataset.save_to_disk(\"filtered_test_dataset\")\n",
    "\n",
    "#-------------------Then manually upload the files to the your Hugging Face repo------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Alternatively, you can upload the dataset directly to your Hugging Face repo-------------------------\n",
    "# from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# # Define repo ID and target folder\n",
    "# repo_id = \"yourusername/datasetname\"  # Change this to your repository\n",
    "# target_folder = \"data\"  # Directly save in 'data' folder\n",
    "\n",
    "# # Save dataset directly in the 'data' folder\n",
    "# filtered_test_dataset.save_to_disk(target_folder)\n",
    "\n",
    "# # Authenticate if needed\n",
    "# if not HfFolder.get_token():\n",
    "#     from huggingface_hub import notebook_login\n",
    "#     notebook_login()\n",
    "\n",
    "# # Upload dataset to the 'data' folder inside the Hugging Face repo\n",
    "# api = HfApi()\n",
    "# api.upload_folder(\n",
    "#     folder_path=target_folder,  # Upload the 'data' folder directly\n",
    "#     repo_id=repo_id,\n",
    "#     repo_type=\"dataset\",\n",
    "#     path_in_repo=\"data\"  # Ensures files are placed inside 'data/'\n",
    "# )\n",
    "\n",
    "# print(f\"Dataset uploaded to: https://huggingface.co/datasets/{repo_id}/tree/main/data\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
