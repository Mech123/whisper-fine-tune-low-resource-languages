# ğŸ”Š Breaking Language Barriers: Fine-Tuning Whisper for Bengali and Telugu ASR

This project fine-tunes OpenAIâ€™s Whisper model for **automatic speech recognition (ASR)** in low-resource Indian languagesâ€”**Bengali** and **Telugu**â€”using parameter-efficient fine-tuning (PEFT) methods such as **LoRA**, **BitFit**, and **Adapter Layers**.

---

## ğŸ§  Core Libraries & Technologies

The following LLM and deep learning libraries were used in this project:

- `openai/whisper` â€“ Pretrained ASR model for multilingual audio
- `transformers` â€“ Model interface & tokenization (Hugging Face)
- `datasets` â€“ Data loading and preprocessing (Hugging Face)
- `peft` â€“ Parameter-efficient fine-tuning (LoRA, BitFit, Adapters)
- `torchaudio` â€“ Audio loading, processing, spectrograms
- `pytorch` â€“ Core deep learning framework for training
- `accelerate` â€“ Easy multi-GPU and mixed-precision training
- `tensorboard` â€“ Real-time training and evaluation tracking

---

## ğŸ“Œ Project Highlights

- ğŸ” Fine-tunes **Whisper Small** using LoRA, BitFit, Adapter Layers
- ğŸ“ˆ Measures Word Error Rate (WER) on Bengali & Telugu ASR tasks
- ğŸ§ª Uses SpecAugment for better generalization
- ğŸ› ï¸ Includes a CLI pipeline for dataset loading, training, and evaluation
- ğŸ§© Designed to be modular, extensible, and reproducible

---

## ğŸš€ Quick Start

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/whisper-fine-tune-low-resource-languages.git
cd whisper-fine-tune-low-resource-languages
