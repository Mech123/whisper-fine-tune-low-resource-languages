{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets>=2.6.1\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install librosa\n",
    "!pip install evaluate>=0.30\n",
    "!pip install jiwer\n",
    "!pip install gradio\n",
    "!pip install -q bitsandbytes datasets accelerate loralib\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main\n",
    "!pip install transformers==4.45.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data, Create Base Whisper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "#---------------------------Load Dataset--------------------------------------\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# Load the full dataset\n",
    "full_train = load_dataset(\"imonghose/bengali-asr-data\", split=\"train\")\n",
    "full_test = load_dataset(\"imonghose/bengali-asr-data\", split=\"test\")\n",
    "\n",
    "\n",
    "\n",
    "# Get dataset sizes\n",
    "train_size = len(full_train)\n",
    "test_size = len(full_test)\n",
    "\n",
    "# Compute the original train-test ratio\n",
    "original_ratio = test_size / train_size\n",
    "\n",
    "# Compute the new test size corresponding to 60% train size\n",
    "new_train_size = int(0.05 * train_size)\n",
    "new_test_size = int(0.2 * test_size)\n",
    "\n",
    "train_sample = full_train.shuffle(seed=42).select(range(new_train_size))\n",
    "test_sample = full_test.shuffle(seed=42).select(range(new_test_size))\n",
    "\n",
    "# Create a new dataset dictionary\n",
    "bangla_dataset = DatasetDict({\n",
    "    \"train\": train_sample,\n",
    "    \"test\": test_sample\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (7978, 3), 'test': (2042, 3)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------Check dataset shape----------\n",
    "bangla_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'sentence', 'length'],\n",
       "        num_rows: 7978\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'sentence', 'length'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------Check dataset structure----------\n",
    "bangla_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': None,\n",
       "  'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         -3.05175781e-05,  0.00000000e+00,  0.00000000e+00]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': 'এরফলে বাংলাদেশ দল এশিয়ান গেমসে প্রথম স্বর্ণপদক লাভ করে।',\n",
       " 'length': 6.12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#-----------Check dataset sample----------\n",
    "bangla_dataset['test'][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------Set model properties-----------------------------------\n",
    "model_name_or_path = \"openai/whisper-small\"\n",
    "language = \"bengali\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path,language=language,task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path,language=language,task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------Downsample to 16 KHz--------------------------\n",
    "from datasets import Audio\n",
    "\n",
    "bangla_dataset = bangla_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'সালাহউদ্দিনের জীবনী লেখার জন্য তিনি খ্যাত।'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check a test transcript\n",
    "bangla_dataset['test'][7]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'পদ্মা সেতু প্রকল্প পরিচালক'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check a train transcript\n",
    "bangla_dataset['train'][16]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50258, 50302, 50359, 50363, 29045, 237, 220, 29045, 116, 29045, 123, 156, 12811, 29045, 123, 29045, 250, 156, 100, 229, 220, 29045, 97, 29045, 123, 29045, 101, 29045, 123, 220, 29045, 227, 29045, 116, 29045, 122, 29045, 100, 29045, 122, 156, 12811, 29045, 96, 220, 29045, 243, 156, 100, 235, 156, 12811, 156, 11061, 29045, 94, 156, 15773, 29045, 122, 29045, 114, 156, 8667, 29045, 110, 156, 11061, 220, 29045, 103, 156, 100, 235, 156, 12811, 29045, 99, 156, 12811, 156, 100, 235, 29045, 114, 29045, 101, 156, 100, 229, 220, 29045, 116, 29045, 243, 156, 100, 235, 29045, 115, 29045, 106, 29045, 97, 29045, 122, 220, 29045, 99, 156, 100, 229, 29045, 244, 29045, 122, 29045, 101, 8703, 97, 50257], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#verify tokenization\n",
    "tokenized_text = tokenizer(bangla_dataset['test'][2]['sentence'], return_tensors=None)\n",
    "tokenized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------Custom preprocessing function for dataset-----------------------------\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "    # encode target text to label ids\n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Execute preprocessing---------------------------\n",
    "bangla_dataset_processed = bangla_dataset.map(prepare_dataset, remove_columns=bangla_dataset.column_names[\"train\"], num_proc=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 7978\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_features', 'labels'],\n",
       "        num_rows: 2042\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------Prepossessed dataset structure-------------------\n",
    "bangla_dataset_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'এটা রাষ্ট্রীয়ভাবে নির্ধারিত হয়ে থাকে।'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#--------------------Regenerate transcription from tokenized text-------------------\n",
    "bangla_dataset_processed['test'][45]['labels']\n",
    "target_text = tokenizer.decode(bangla_dataset_processed['test'][45]['labels'], skip_special_tokens=True)\n",
    "target_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test predictions for base Whisper model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : রমার স্বামী দিবানাথ সেন \n",
      "Pred :  Rāmārṣāmi divanātṣen\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : বাংলাদেশের অভিজ্ঞ এ পেসার \n",
      "Pred :  बाग्णादिश रोभीक को एप प्रेसर\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : ব্যাসদেব এক দিন \n",
      "Pred :  बेस्देप एक दिन\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : আইসিসির অন্তত \n",
      "Pred :  I see here on total\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences before fine-tuning (Train set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"train\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"train\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : কলেজটি প্রতিষ্ঠা হবার পর নানান সময়ে সমালোচিত হয়েছে। \n",
      "Pred :  ្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្្\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : এটি পরিচালনা করেছিলেন তামিল চলচ্চিত্র শিল্পের পরিচালক মণি রত্নম। \n",
      "Pred :  ಸಿರಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿಕಿ\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : এদের প্রাথমিক খাদ্য ইঁদুর জাতীয় ক্ষুদ্র-তীক্ষ্ণদন্ত প্রাণী। \n",
      "Pred :  एदे प्रात्मिक खाद्द, इदूर जातियो खुद्रो तिखनू दान्तो प्रानि\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : পশ্চিমবঙ্গের জেলা ম্যাজিস্ট্রেটের তিনটি পদ এই কর্মকর্তাদের জন্য সংরক্ষিত। \n",
      "Pred :  ಸಾರಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿಲಿ\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences before fine-tuning (Test set) ------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"test\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"test\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = model.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions for fine tuned Whisper model (LORA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:168: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Load LORA model from Hugging Face Hub-----------------------------------\n",
    "\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "\n",
    "# Load base model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "peft_model = get_peft_model(base_model, config)\n",
    "\n",
    "# Load LoRA adapter\n",
    "final_loaded_model_lora = PeftModel.from_pretrained(peft_model, \"imonghose/whisper-small-bengali-lora-final\")\n",
    "# final_loaded_model = PeftModel.from_pretrained(peft_model, \"trained_model\")\n",
    "\n",
    "# ✅ Move model to GPU\n",
    "final_loaded_model_lora = final_loaded_model_lora.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : রমার স্বামী দিবানাথ সেন \n",
      "Pred : ডমারসামে দিভানাতিসান\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : বাংলাদেশের অভিজ্ঞ এ পেসার \n",
      "Pred : মাংনাদেশ্ষ্য়ভিক্ক এ প্রেসার\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : ব্যাসদেব এক দিন \n",
      "Pred : ভেস দেপ একদেন\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : আইসিসির অন্তত \n",
      "Pred : আয়িস্যর অন তত\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences after fine-tuning (LORA) (Train set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"train\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"train\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_lora.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : কলেজটি প্রতিষ্ঠা হবার পর নানান সময়ে সমালোচিত হয়েছে। \n",
      "Pred : কলেস টিপতিসতা হাভার পর নানছমায় সমলেত দো হয়েছে\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : এটি পরিচালনা করেছিলেন তামিল চলচ্চিত্র শিল্পের পরিচালক মণি রত্নম। \n",
      "Pred : এটি পরিচলানকরিছিলান তামিলছলছিতে সিলেরের পরিচলও মনীরাতনম\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : এদের প্রাথমিক খাদ্য ইঁদুর জাতীয় ক্ষুদ্র-তীক্ষ্ণদন্ত প্রাণী। \n",
      "Pred : এদের প্ধমিক হাত্ত ইদুুর জাতিয় খুরে তিখ্যকন্ত পরানে\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : পশ্চিমবঙ্গের জেলা ম্যাজিস্ট্রেটের তিনটি পদ এই কর্মকর্তাদের জন্য সংরক্ষিত। \n",
      "Pred : পুশিম গে জেলা মাইজেস্টেছ তিমতি পাতে কাল্ম কাদ্রদ্য়ে সন্রো ফীর্যব্যর\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences after fine-tuning (LORA) (Test set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"test\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"test\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_lora.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions for fine tuned Whisper model (LORA + SpecAugment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\peft\\mapping_func.py:79: UserWarning: The PEFT config's `base_model_name_or_path` was renamed from 'openai/whisper-small' to 'None'. Please ensure that the correct base model is loaded when loading this checkpoint.\n",
      "  warnings.warn(\n",
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:168: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Load LORA+SpecAugment model from Hugging Face Hub-----------------------------------\n",
    "\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "\n",
    "# Load base model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "peft_model = get_peft_model(base_model, config)\n",
    "peft_model = get_peft_model(peft_model, config)\n",
    "# loaded_model = get_peft_model(loaded_model, \"trained_model\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "final_loaded_model_lora_spec = PeftModel.from_pretrained(peft_model, \"imonghose/whisper-small-bengali-lora-specaugment-final\")\n",
    "\n",
    "# ✅ Move model to GPU\n",
    "final_loaded_model_lora_spec = final_loaded_model_lora_spec.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : রমার স্বামী দিবানাথ সেন \n",
      "Pred : ডমারসামে দিভানাতশান\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : বাংলাদেশের অভিজ্ঞ এ পেসার \n",
      "Pred : মাংনাদেশ্ষ্য়ভিকব এ প্রেসার\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : ব্যাসদেব এক দিন \n",
      "Pred : ভেস দেপ একদেন\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : আইসিসির অন্তত \n",
      "Pred : আইসিস্য়র অনতত\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#--------Check true vs prediction on few sentences after fine-tuning (LORA+SpecAugment) (Train set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"train\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"train\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_lora_spec.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : কলেজটি প্রতিষ্ঠা হবার পর নানান সময়ে সমালোচিত হয়েছে। \n",
      "Pred : কলেসী পতিষতা হাভ্র পর নানছমায় সমলেত দোয়েছে\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : এটি পরিচালনা করেছিলেন তামিল চলচ্চিত্র শিল্পের পরিচালক মণি রত্নম। \n",
      "Pred : এটি পর্যছলানকরিছিলান তামিলছলছিতের সিলেল্পর পরিচলও মনীরাতনম\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : এদের প্রাথমিক খাদ্য ইঁদুর জাতীয় ক্ষুদ্র-তীক্ষ্ণদন্ত প্রাণী। \n",
      "Pred : এদের প্ধমিকাত্ত ইদুর জাতিয় খুর্যর তিখন্ত পরানে\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : পশ্চিমবঙ্গের জেলা ম্যাজিস্ট্রেটের তিনটি পদ এই কর্মকর্তাদের জন্য সংরক্ষিত। \n",
      "Pred : পুশিম গে জেলা মাইজেশ্য়তেছ তিমতপাতে কাল্ম কাদ্য়্য় সঙন্র্পীর্য্যর\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#-------------Check true vs prediction on few sentences after fine-tuning (LORA+SpecAugment) (Test set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"test\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"test\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_lora_spec.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions for fine tuned Whisper model (BitFit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\imong\\.cache\\huggingface\\hub\\models--imonghose--whisper-small-bengali-bitfit-final. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Load BitFit model from Hugging Face Hub-----------------------------------\n",
    "final_loaded_model_bitfit = WhisperForConditionalGeneration.from_pretrained(\"imonghose/whisper-small-bengali-bitfit-final\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : রমার স্বামী দিবানাথ সেন \n",
      "Pred : সামাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযা\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : বাংলাদেশের অভিজ্ঞ এ পেসার \n",
      "Pred : সাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযা\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : ব্যাসদেব এক দিন \n",
      "Pred : সালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালালা\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : আইসিসির অন্তত \n",
      "Pred : আইসাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযাযা\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences after fine-tuning (BitFit) (Train set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"train\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"train\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_bitfit.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : কলেজটি প্রতিষ্ঠা হবার পর নানান সময়ে সমালোচিত হয়েছে। \n",
      "Pred : কালেসেকামানালেকালেকালেকালেকালেকালে\n",
      "\n",
      " \n",
      "-------4------\n",
      "True : এটি পরিচালনা করেছিলেন তামিল চলচ্চিত্র শিল্পের পরিচালক মণি রত্নম। \n",
      "Pred : কাতেপেয়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়ায়\n",
      "\n",
      " \n",
      "-------5------\n",
      "True : এদের প্রাথমিক খাদ্য ইঁদুর জাতীয় ক্ষুদ্র-তীক্ষ্ণদন্ত প্রাণী। \n",
      "Pred : কানানানাকিকি ইদনিকিককা ইদিককি তানানানানা\n",
      "\n",
      " \n",
      "-------6------\n",
      "True : পশ্চিমবঙ্গের জেলা ম্যাজিস্ট্রেটের তিনটি পদ এই কর্মকর্তাদের জন্য সংরক্ষিত। \n",
      "Pred : কাসিনিনি জলা সিনা নান তাতা কাক ককা ককা ককা ককা ককা ককা ককা ককা ককা ককা ককা ককক ককা ককক ককক ককক ককক ককক ককক ককক কক কক কক কক কক কক কক কক কক কক কক কক ক কক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক ক \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#------------------------------Check true vs prediction on few sentences after fine-tuning (BitFit) (Test set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"test\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"test\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_bitfit.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Predictions for fine tuned Whisper model (Adapter Layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Create a custom Whisper class for injecting adapter layers----------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, hidden_size, adapter_dim=64):\n",
    "        super().__init__()\n",
    "        self.down_proj = nn.Linear(hidden_size, adapter_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.up_proj = nn.Linear(adapter_dim, hidden_size)\n",
    "\n",
    "        # Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.down_proj.weight)\n",
    "        nn.init.zeros_(self.down_proj.bias)\n",
    "        nn.init.xavier_uniform_(self.up_proj.weight)\n",
    "        nn.init.zeros_(self.up_proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.up_proj(self.activation(self.down_proj(x)))\n",
    "\n",
    "\n",
    "class WhisperWithAdapters(WhisperForConditionalGeneration):\n",
    "    def __init__(self, config, adapter_dim=64):\n",
    "        super().__init__(config)\n",
    "        self.adapter_dim = adapter_dim\n",
    "        self._add_adapters()\n",
    "\n",
    "    def _add_adapters(self):\n",
    "        # Encoder adapters\n",
    "        for layer in self.model.encoder.layers:\n",
    "            hidden_size = layer.self_attn.embed_dim\n",
    "            layer.adapter = Adapter(hidden_size, self.adapter_dim)\n",
    "\n",
    "            original_forward = layer.forward\n",
    "\n",
    "            def patched_forward(self, hidden_states, attention_mask=None, output_attentions=False, **kwargs):\n",
    "                outputs = original_forward(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                    **kwargs\n",
    "                )\n",
    "                hidden_states = outputs[0]\n",
    "                hidden_states = self.adapter(hidden_states)\n",
    "                return (hidden_states,) + outputs[1:]\n",
    "\n",
    "            layer.forward = patched_forward.__get__(layer, type(layer))\n",
    "\n",
    "        # Decoder adapters\n",
    "        for layer in self.model.decoder.layers:\n",
    "            hidden_size = layer.self_attn.embed_dim\n",
    "            layer.adapter = Adapter(hidden_size, self.adapter_dim)\n",
    "\n",
    "            original_forward = layer.forward\n",
    "\n",
    "            def patched_forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None,\n",
    "                                encoder_attention_mask=None, output_attentions=False, **kwargs):\n",
    "                outputs = original_forward(\n",
    "                    hidden_states,\n",
    "                    attention_mask=attention_mask,\n",
    "                    encoder_hidden_states=encoder_hidden_states,\n",
    "                    encoder_attention_mask=encoder_attention_mask,\n",
    "                    output_attentions=output_attentions,\n",
    "                    **kwargs\n",
    "                )\n",
    "                hidden_states = outputs[0]\n",
    "                hidden_states = self.adapter(hidden_states)\n",
    "                return (hidden_states,) + outputs[1:]\n",
    "\n",
    "            layer.forward = patched_forward.__get__(layer, type(layer))\n",
    "\n",
    "    def freeze_all_but_adapters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for name, module in self.named_modules():\n",
    "            if 'adapter' in name:\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = True\n",
    "\n",
    "    def save_pretrained(self, save_directory, **kwargs):\n",
    "        self.config.adapter_dim = self.adapter_dim\n",
    "        super().save_pretrained(save_directory, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, config=None, **kwargs):\n",
    "        from transformers import AutoConfig\n",
    "        if config is None:\n",
    "            config = AutoConfig.from_pretrained(pretrained_model_name_or_path)\n",
    "        adapter_dim = getattr(config, \"adapter_dim\", 64)\n",
    "        model = super().from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
    "        model.__class__ = cls\n",
    "        model.adapter_dim = adapter_dim\n",
    "        model._add_adapters()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\imong\\.cache\\huggingface\\hub\\models--imonghose--whisper-small-bengali-adapter-layer-final. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Load Adapter Layers model from Hugging Face Hub-----------------------------------\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "final_loaded_model_adapter = WhisperWithAdapters.from_pretrained(\"imonghose/whisper-small-bengali-adapter-layer-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Move model to GPU\n",
    "final_loaded_model_adapter = final_loaded_model_adapter.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------3------\n",
      "True : রমার স্বামী দিবানাথ সেন \n",
      "Pred : \n",
      "\n",
      " \n",
      "-------4------\n",
      "True : বাংলাদেশের অভিজ্ঞ এ পেসার \n",
      "Pred : \n",
      "\n",
      " \n",
      "-------5------\n",
      "True : ব্যাসদেব এক দিন \n",
      "Pred : \n",
      "\n",
      " \n",
      "-------6------\n",
      "True : আইসিসির অন্তত \n",
      "Pred : \n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#--------Check true vs prediction on few sentences after fine-tuning (Adapter Layers) (Train set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"train\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"train\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_adapter.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------Check true vs prediction on few sentences after fine-tuning (Adapter Layers) (Test set)------------------\n",
    "import torch\n",
    "\n",
    "for idx in range(3,7):\n",
    "    # Get the tokenized target labels\n",
    "    target_tokenized = bangla_dataset_processed[\"test\"][idx][\"labels\"]\n",
    "\n",
    "    # Decode the true text from tokenized format\n",
    "    target_text = tokenizer.decode(target_tokenized, skip_special_tokens=True)\n",
    "\n",
    "    # Convert input features to tensor and add batch dimension\n",
    "    input_feature = torch.tensor(bangla_dataset_processed[\"test\"][idx][\"input_features\"]).unsqueeze(0)\n",
    "\n",
    "    # Ensure correct data type and move to GPU\n",
    "    input_feature = input_feature.to(dtype=torch.float32, device='cuda')  # Use float16 if needed\n",
    "\n",
    "    with torch.no_grad():\n",
    "        op = final_loaded_model_adapter.generate(input_feature, language='bengali', task='transcribe')\n",
    "\n",
    "    # Decode predicted text\n",
    "    text_pred = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(f'-------{idx}------')\n",
    "    print(f'True : {target_text} \\nPred : {text_pred}')\n",
    "    print('\\n ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
