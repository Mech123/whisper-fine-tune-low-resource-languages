{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Speech Diarization with Whisper (HF) + pyannote\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "from pyannote.audio import Pipeline\n",
    "import os\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 1. Load Local WAV File --------------\n",
    "AUDIO_FILE = \"bengali-convo-2.wav\"  # replace with your file\n",
    "\n",
    "# Preview audio\n",
    "ipd.Audio(AUDIO_FILE)\n",
    "\n",
    "# Load audio\n",
    "waveform, sr = torchaudio.load(AUDIO_FILE)\n",
    "\n",
    "waveform = waveform[0].numpy()  # mono\n",
    "resampled = librosa.resample(waveform, orig_sr=sr, target_sr=16000)\n",
    "sr = 16000  # Whisper expects 16kHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------Set model properties-----------------------------------\n",
    "model_name_or_path = \"openai/whisper-small\"\n",
    "language = \"bengali\"\n",
    "task = \"transcribe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path,language=language,task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperTokenizer\n",
    "\n",
    "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path,language=language,task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\IMON\\Masters\\DKE Course\\Semester 3\\HCNLP\\FINE_TUNE_WHISPER\\whisper-env\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:168: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Load LORA model from Hugging Face Hub-----------------------------------\n",
    "\n",
    "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"k_proj\", \"v_proj\", \"q_proj\", \"out_proj\"], lora_dropout=0.05, bias=\"none\")\n",
    "\n",
    "\n",
    "# Load base model\n",
    "base_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "peft_model = get_peft_model(base_model, config)\n",
    "\n",
    "# Load LoRA adapter\n",
    "fine_tuned_model = PeftModel.from_pretrained(peft_model, \"imonghose/whisper-small-bengali-lora-final\")\n",
    "# final_loaded_model = PeftModel.from_pretrained(peft_model, \"trained_model\")\n",
    "\n",
    "# ‚úÖ Move model to GPU\n",
    "fine_tuned_model = fine_tuned_model.to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- 2. Load Whisper model from Hugging Face --------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "model = fine_tuned_model\n",
    "\n",
    "# Prepare input\n",
    "inputs = processor(resampled, sampling_rate=sr, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "# Generate token ids\n",
    "with torch.no_grad():\n",
    "    op = model.generate(inputs, language='bengali', task='transcribe')\n",
    "transcription = tokenizer.batch_decode(op, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"Full Transcription:\")\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------Manually provide correct transcription for testing diarization-------------------\n",
    "transcription = '‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßã ‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶≤‡¶æ ‡¶è‡¶®‡ßã ‡¶†‡¶ø‡¶ï ‡¶Ü‡¶õ‡ßá, ‡¶®‡¶ø‡¶Ø‡¶º‡ßá ‡¶Ü‡¶∏‡¶¨‡ßã‡•§'\n",
    "# transcription = '‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡ßá‡¶Æ‡¶® ‡¶Ü‡¶õ‡ßã ‡¶Ü‡¶ú ‡¶ï‡¶ø ‡¶ï‡¶∞‡¶õ‡ßã ‡¶≠‡¶æ‡¶≤‡ßã ‡¶Ü‡¶õ‡¶ø ‡¶¨‡¶á ‡¶™‡¶°‡¶º‡¶õ‡¶ø ‡¶ï‡ßÄ ‡¶¨‡¶á ‡¶™‡¶°‡¶º‡¶õ‡ßã ‡¶ó‡¶≤‡ßç‡¶™ ‡¶®‡¶æ‡¶ï‡¶ø ‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏ ‡¶â‡¶™‡¶®‡ßç‡¶Ø‡¶æ‡¶∏ ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶≤‡¶æ‡¶ó‡¶õ‡ßá'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\imong\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu124. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. Diarization using pyannote.audio --------------\n",
    "\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "hf_token = \"your_huggingface_token_here\"\n",
    "\n",
    "diarization_pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization\",\n",
    "    use_auth_token=hf_token\n",
    ")\n",
    "\n",
    "# Perform diarization\n",
    "diarization_result = diarization_pipeline(AUDIO_FILE,num_speakers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<Segment(1.78597, 3.10222)>, 'A', 'SPEAKER_00'),\n",
       " (<Segment(3.27097, 4.53659)>, 'B', 'SPEAKER_01'),\n",
       " (<Segment(4.53659, 7.91159)>, 'C', 'SPEAKER_00'),\n",
       " (<Segment(7.91159, 8.90722)>, 'D', 'SPEAKER_01')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_segments = list(diarization_result.itertracks(yield_label=True))\n",
    "speaker_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imong\\AppData\\Local\\Temp\\ipykernel_97244\\2406932817.py:6: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename=AUDIO_FILE)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyannote.core import Segment\n",
    "import librosa\n",
    "\n",
    "# Get total duration of the audio\n",
    "duration = librosa.get_duration(filename=AUDIO_FILE)\n",
    "\n",
    "# Split words and estimate per-word duration\n",
    "words = transcription.split()\n",
    "total_words = len(words)\n",
    "avg_word_duration = duration / total_words\n",
    "\n",
    "# Diarization segments\n",
    "speaker_segments = list(diarization_result.itertracks(yield_label=True))\n",
    "\n",
    "# Make sure we have diarization segments\n",
    "if not speaker_segments:\n",
    "    raise ValueError(\"No speaker segments found in diarization result.\")\n",
    "\n",
    "# Get start of first speaker segment\n",
    "first_speaker_start = min([segment[0].start for segment in speaker_segments])\n",
    "\n",
    "# Build timestamped word list (shifted to match first diarization start)\n",
    "word_times = []\n",
    "for i, word in enumerate(words):\n",
    "    start_time = first_speaker_start + i * avg_word_duration\n",
    "    end_time = first_speaker_start + (i + 1) * avg_word_duration\n",
    "    word_times.append((word, start_time, end_time))\n",
    "\n",
    "# Align each word to the correct speaker based on diarization\n",
    "speaker_words = []\n",
    "for word, start, end in word_times:\n",
    "    for (seg_start, seg_end), _, speaker in speaker_segments:\n",
    "        if seg_start <= start < seg_end:\n",
    "            speaker_words.append((speaker, word))\n",
    "            break\n",
    "\n",
    "# Group consecutive words by speaker\n",
    "final_output = []\n",
    "if speaker_words:\n",
    "    current_speaker, current_words = speaker_words[0][0], []\n",
    "    for speaker, word in speaker_words:\n",
    "        if speaker == current_speaker:\n",
    "            current_words.append(word)\n",
    "        else:\n",
    "            final_output.append((current_speaker, \" \".join(current_words)))\n",
    "            current_speaker = speaker\n",
    "            current_words = [word]\n",
    "    final_output.append((current_speaker, \" \".join(current_words)))  # last group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üó£Ô∏è Final Diarized Transcript:\n",
      "\n",
      "SPEAKER_00 : \"‡¶ï‡ßã‡¶•‡¶æ‡¶Ø‡¶º ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡ßã\"\n",
      "SPEAKER_01 : \"‡¶¨‡¶æ‡¶ú‡¶æ‡¶∞‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø\"\n",
      "SPEAKER_00 : \"‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡¶≤‡¶æ ‡¶è‡¶®‡ßã\"\n",
      "SPEAKER_01 : \"‡¶†‡¶ø‡¶ï\"\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. Display Final Result --------------\n",
    "\n",
    "print(\"\\nüó£Ô∏è Final Diarized Transcript:\\n\")\n",
    "for speaker, text in final_output:\n",
    "    print(f\"{speaker} : \\\"{text}\\\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
